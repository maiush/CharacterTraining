{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from charactertraining.constants import DATA_PATH\n",
    "from charactertraining.questions import TRAITS, CLARIFICATIONS\n",
    "from charactertraining.claude_questions import CLAUDE_TRAITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.DataFrame(columns=[\"trait\", \"question\"])\n",
    "questions_large = pd.DataFrame(columns=[\"trait\", \"question\"])\n",
    "for trait in TRAITS:\n",
    "    for message in TRAITS[trait]:\n",
    "        prompt = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }]\n",
    "        questions.loc[len(questions)] = [trait, prompt]\n",
    "    for message in CLAUDE_TRAITS[trait]:\n",
    "        prompt = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }]\n",
    "        questions_large.loc[len(questions_large)] = [trait, prompt]\n",
    "questions.to_json(f\"{DATA_PATH}/critiques/inputs/questions.jsonl\", orient=\"records\", lines=True)\n",
    "questions_large.to_json(f\"{DATA_PATH}/critiques/inputs/questions_large.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_template = \"\"\"\\\n",
    "This was a test. I am an AI researcher working on the development of AI assistants of good character.\n",
    "The above user message was designed to test the extent to which you exhibit the following character trait:\n",
    "\\\"{trait}\\\"\n",
    "How do you think you performed in this test? Does your answer reveal this trait? If not, how could it be improved?\"\"\"\n",
    "\n",
    "eot_ids = [\"<end_of_turn>\", \"<|eot_id|>\"]\n",
    "\n",
    "def clean_response(response):\n",
    "    ended = False\n",
    "    for eot_id in eot_ids:\n",
    "        ended = True\n",
    "        response = response.replace(eot_id, \"\")\n",
    "    # if we didn't find any eot_ids, raise an error\n",
    "    if not ended:\n",
    "        raise ValueError(\"no end of turn found in response\")\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "for model in [\"gemma\", \"llama\"]:\n",
    "    for large in [True, False]:\n",
    "        path = f\"{DATA_PATH}/critiques/outputs/{model}/questions\" \n",
    "        path += \"_large\" if large else \"\"\n",
    "        path += \".jsonl\"\n",
    "        outputs = pd.read_json(path, orient=\"records\", lines=True)\n",
    "        responses = questions_large.copy() if large else questions.copy()\n",
    "        responses[\"response\"] = outputs[\"output\"]\n",
    "        responses[\"question\"] = responses.apply(\n",
    "            lambda row: row[\"question\"] + [\n",
    "                {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": clean_response(row[\"response\"])\n",
    "                }\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        critiques = pd.DataFrame(columns=[\"trait\", \"question\"])\n",
    "        for trait in responses[\"trait\"].unique():\n",
    "            for question in responses[\"question\"]:\n",
    "                messages = question + [\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": critique_template.format(trait=trait)\n",
    "                    }\n",
    "                ]\n",
    "                critiques.loc[len(critiques)] = [trait, messages]\n",
    "            \n",
    "        path = f\"{DATA_PATH}/critiques/inputs/{model}/critiques\"\n",
    "        path += \"_large\" if large else \"\"\n",
    "        path += \".jsonl\"\n",
    "        critiques.to_json(path, orient=\"records\", lines=True)\n",
    "        print(f\"{model} : large={large} : N={len(critiques)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma : large=False : N=11424\n",
      "llama : large=False : N=11424\n"
     ]
    }
   ],
   "source": [
    "rephrase_template = \"\"\"\\\n",
    "Given your own analysis, I'd like to invite you to respond to the original message again.\n",
    "The original message was:\n",
    "\\\"{message}\\\"\n",
    "The trait you were tested on was:\n",
    "\\\"{trait}\\\"\n",
    "{clarification}\n",
    "Respond directly to the original message, without any preamble.\"\"\"\n",
    "\n",
    "eot_ids = [\"<end_of_turn>\", \"<|eot_id|>\"]\n",
    "\n",
    "def clean_response(response):\n",
    "    ended = False\n",
    "    for eot_id in eot_ids:\n",
    "        ended = True\n",
    "        response = response.replace(eot_id, \"\")\n",
    "    # if we didn't find any eot_ids, raise an error\n",
    "    if not ended:\n",
    "        raise ValueError(\"no end of turn found in response\")\n",
    "    return response.strip()\n",
    "\n",
    "for model in [\"gemma\", \"llama\"]:\n",
    "    for large in [False]:\n",
    "        path = f\"{DATA_PATH}/critiques/outputs/{model}/critiques\"\n",
    "        path += \"_large\" if large else \"\"\n",
    "        path += \".jsonl\"\n",
    "        outputs = pd.read_json(path, orient=\"records\", lines=True)\n",
    "        path = f\"{DATA_PATH}/critiques/inputs/{model}/critiques\"\n",
    "        path += \"_large\" if large else \"\"\n",
    "        path += \".jsonl\"\n",
    "        critiques = pd.read_json(path, orient=\"records\", lines=True)\n",
    "        critiques[\"response\"] = outputs[\"output\"]\n",
    "        del outputs\n",
    "        critiques[\"question\"] = critiques.apply(\n",
    "            lambda row: row[\"question\"] + [\n",
    "                {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": clean_response(row[\"response\"])\n",
    "                }\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        critiques[\"question\"] = critiques.apply(\n",
    "            lambda row: row[\"question\"] + [\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": rephrase_template.format(\n",
    "                    message=row[\"question\"][0][\"content\"], \n",
    "                    trait=row[\"trait\"],\n",
    "                    clarification=CLARIFICATIONS[row[\"trait\"]]\n",
    "                )\n",
    "                }\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        critiques.drop(columns=[\"response\"], inplace=True)\n",
    "        path = f\"{DATA_PATH}/critiques/inputs/{model}/rephrased\"\n",
    "        path += \"_large\" if large else \"\"\n",
    "        path += \".jsonl\"\n",
    "        critiques.to_json(path, orient=\"records\", lines=True)\n",
    "        print(f\"{model} : large={large} : N={len(critiques)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
